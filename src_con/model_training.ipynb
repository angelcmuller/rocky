{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04834f3-888b-4bbf-931e-be7f328c23d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Author: Tristan Bailey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d8981-5a48-49a2-b4b1-93b906389137",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73300be9-b96e-46a2-bfdc-c473ab907eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d26644-7f36-480d-88c8-361a2f6bc72e",
   "metadata": {},
   "source": [
    "Verify System has Cuda Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff594982-c11b-4874-8a75-118296ab6b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "has_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"Will utilize GPU acceleration\")\n",
    "    has_cuda = True\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a076281-8d73-4d9e-a644-efbd5c3693a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0 : \"Bump\",\n",
    "    1 : \"Crack\",\n",
    "    2 : \"Plain\",\n",
    "    3 : \"Pot Hole\",\n",
    "    4 : \"Speed Bump\"\n",
    "}\n",
    "checkpoint_interval = 1\n",
    "num_classes = 5\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "model_name = \"Resnet_10batch_grayscale.pth\"\n",
    "checkpoint_dir = \"checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d90e5-538f-4f03-97c2-d57c22dacda4",
   "metadata": {},
   "source": [
    "Training Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4cf99-332b-47bc-98d5-2d51259e83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "# The folder directory that has the image folders\n",
    "data_dir_name = \"training_data\"\n",
    "data_path = os.path.join(cwd, data_dir_name)\n",
    "training_data_path = data_path + \"_grayscale\" + \"_train\"\n",
    "testing_data_path = data_path + \"_grayscale\" + \"_test\"\n",
    "model_path = os.path.join(cwd, model_name)\n",
    "checkpoint_dir = os.path.join(os.getcwd(), checkpoint_dir)\n",
    "print(training_data_path)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6150bdc-e789-4ba9-8aa3-9b6b18af5501",
   "metadata": {},
   "source": [
    "Data Selection (ONLY DO ONCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e498d9-2273-4b94-b3a7-316422dcd45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN ONCE, needed to clean names for pytorch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the new directory name with \"_grayscale\" added\n",
    "grayscale_data_path = data_path + \"_grayscale\"\n",
    "\n",
    "# Create the new directory if it doesn't exist\n",
    "if not os.path.exists(grayscale_data_path):\n",
    "    os.makedirs(grayscale_data_path)\n",
    "\n",
    "# Iterate through subdirectories\n",
    "for subdir, _, _ in os.walk(data_path):\n",
    "    # Create a new subdirectory in the grayscale directory with the same name\n",
    "    new_subdir = subdir.replace(data_path, grayscale_data_path)\n",
    "    if not os.path.exists(new_subdir):\n",
    "        os.makedirs(new_subdir)\n",
    "        \n",
    "    for filename in os.listdir(subdir):\n",
    "        # Check if it's a file\n",
    "        if os.path.isfile(os.path.join(subdir, filename)):\n",
    "            # Get file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "            \n",
    "            # Open the image and convert it to grayscale\n",
    "            img = Image.open(os.path.join(subdir, filename)).convert('L')\n",
    "            \n",
    "            # Check if the image is in palette mode with transparency\n",
    "            if img.mode == \"P\" and \"transparency\" in img.info:\n",
    "                # Convert the image to RGBA mode\n",
    "                img = img.convert(\"RGBA\")\n",
    "                \n",
    "            # Save the grayscale image with the original filename to the new directory\n",
    "            new_filename = filename.replace(file_extension, \"_grayscale.png\")\n",
    "            img.save(os.path.join(new_subdir, new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b31d5-f076-48ca-9b1d-372033ed1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN ONCE, needed to get test and train set\n",
    "\n",
    "import random\n",
    "# Define the new directory path with \"_train\" and \"_test\" added\n",
    "train_path = grayscale_data_path + \"_train\"\n",
    "test_path = grayscale_data_path + \"_test\"\n",
    "\n",
    "# Create the new directories if they don't exist\n",
    "if not os.path.exists(train_path):\n",
    "    os.makedirs(train_path)\n",
    "if not os.path.exists(test_path):\n",
    "    os.makedirs(test_path)\n",
    "\n",
    "# Iterate through subdirectories\n",
    "for subdir, _, filenames in os.walk(grayscale_data_path):\n",
    "    # Get the new subdirectory path with \"_train\" or \"_test\" added\n",
    "    new_subdir_base = subdir.replace(grayscale_data_path, train_path)\n",
    "    new_subdir_test = new_subdir_base.replace(\"_train\", \"_test\")\n",
    "    \n",
    "    # Create the new subdirectories if they don't exist\n",
    "    if not os.path.exists(new_subdir_base):\n",
    "        os.makedirs(new_subdir_base)\n",
    "    if not os.path.exists(new_subdir_test):\n",
    "        os.makedirs(new_subdir_test)\n",
    "    \n",
    "    # Iterate through filenames\n",
    "    for filename in filenames:\n",
    "        # Get the file path\n",
    "        file_path = os.path.join(subdir, filename)\n",
    "        \n",
    "        # Decide whether to save the file in the train or test directory\n",
    "        if random.random() < 0.9:\n",
    "            new_file_path = os.path.join(new_subdir_base, filename)\n",
    "        else:\n",
    "            new_file_path = os.path.join(new_subdir_test, filename)\n",
    "            \n",
    "        # Copy the file to the new directory\n",
    "        with open(file_path, \"rb\") as f_src:\n",
    "            with open(new_file_path, \"wb\") as f_dst:\n",
    "                f_dst.write(f_src.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7c1c9-8704-423f-a590-e3e30c1d7df3",
   "metadata": {},
   "source": [
    "Data Augmentation and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eefe47-c185-4d20-b134-41a4aea4c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 - for RGB baseline\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbfa49-55c3-48a8-b26f-1905a3425238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=None)\n",
    "    ], p=0.5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c95a9e-45a5-48de-8b4b-af181dd01cc0",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a636a-75f9-4b6b-87db-16732c9b74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 & 3\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = datasets.ImageFolder(root=training_data_path, transform=transform)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0c32f-e5ea-4b77-9237-fe02b0cb476d",
   "metadata": {},
   "source": [
    "Display Samples from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b06fb4-f3e5-4f3b-918f-8fd4d8c7ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 - for RGB baseline\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Apply the same transform as used during training\n",
    "inverse_transform = transforms.Compose([\n",
    "    transforms.Normalize([-0.485/0.229, -0.456/0.224, -0.406/0.225], [1/0.229, 1/0.224, 1/0.225]),\n",
    "])\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    \n",
    "    # Apply the inverse transform to display the image correctly\n",
    "    img = inverse_transform(img)\n",
    "    img = to_pil_image(img)  # Convert the tensor back to the PIL image format\n",
    "    img = img.convert('L')   # Convert the image to grayscale mode\n",
    "\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67705f-bf5f-4822-8dd7-d27ca2005904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Apply the same transform as used during training\n",
    "inverse_transform = transforms.Compose([    transforms.Normalize([-0.5/0.5], [1/0.5])\n",
    "])\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    \n",
    "    # Apply the inverse transform to display the image correctly\n",
    "    img = inverse_transform(img)\n",
    "    img = to_pil_image(img)  # Convert the tensor back to the PIL image format\n",
    "    img = img.convert('L')   # Convert the image to grayscale mode\n",
    "\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087db7d0-3a5f-48f6-b657-ec525e64db7f",
   "metadata": {},
   "source": [
    "Import Generic Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1b87b-4c84-447b-8294-4acf1210e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model (ImageNet)\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Remove final classifier layer and replace it with your custom layer\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdeaddd-ea21-4747-8c16-67f1af4f4d8a",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daca5fa-0bc4-4647-ae98-c8bc8063601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 & 3\n",
    "\n",
    "if has_cuda:\n",
    "    resnet = resnet.cuda()\n",
    "\n",
    "# Set loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c971698-e263-4a0e-8574-282848724200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3\n",
    "# Convert the single channel grayscale input to 3-channel input by repeating the same image\n",
    "def convert_to_3channel(inputs):\n",
    "    inputs = inputs.repeat(1, 3, 1, 1)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b9c10-11fd-487a-93a0-e01a6d068137",
   "metadata": {
    "tags": []
   },
   "source": [
    "Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141cfdf-7821-4fd5-9acd-56b137819476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 RGB Baseline\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if has_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    # Checkpoint the model\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_filename = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save(resnet.state_dict(), os.path.join(checkpoint_dir, f\"epoch_{epoch}.pth\"))\n",
    "        print(f\"Checkpoint saved: {checkpoint_filename}\")\n",
    "        with open(\"resnet_accuracy.txt\", \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch + 1}: Accuracy = {epoch_accuracy:.2f}%\\n\")\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9fa19-6108-45cb-9983-d1495cdd2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if has_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        inputs = convert_to_3channel(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    # Checkpoint the model\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_filename = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save(resnet.state_dict(), os.path.join(checkpoint_dir, f\"epoch_{epoch}.pth\"))\n",
    "        print(f\"Checkpoint saved: {checkpoint_filename}\")\n",
    "        with open(\"resnet_accuracy.txt\", \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch + 1}: Accuracy = {epoch_accuracy:.2f}%\\n\")\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c730c-5ea3-41ef-8688-33aa5120d9ee",
   "metadata": {},
   "source": [
    "Save Model as Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e90af-727c-46a0-b141-2a56161ef475",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), model_path)\n",
    "print(f\"Model saved as {model_name} to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8257b10-0cc2-4635-bab4-fdc1a770bf53",
   "metadata": {},
   "source": [
    "Evaluation Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b75ee2-3132-47b8-9b2e-1c323ffe5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add simple transfomr, no augmentation, to test\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(root=testing_data_path, transform=transform)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b960e-b423-4afc-87ea-8b5ebb1001a6",
   "metadata": {},
   "source": [
    "Loading A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ba6b4-8443-4179-9e82-d051d864025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "resnet = models.resnet18(weights=None)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "model_path = \"epoch_29.pth\"\n",
    "resnet.load_state_dict(torch.load(model_path))\n",
    "# If using GPU, move the model to GPU\n",
    "if has_cuda:\n",
    "    resnet = resnet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813b311-a685-4a59-bde3-43db26de8ec5",
   "metadata": {},
   "source": [
    "Testing Model (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafa54e-0f2d-484a-b564-81652d2fdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "# Evaluation loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        if has_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        inputs = convert_to_3channel(inputs)\n",
    "        outputs = resnet(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print the accuracy of the test data\n",
    "print(\"Accuracy on test data: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d970f-cb2d-499e-9fee-11b1f526e9ba",
   "metadata": {},
   "source": [
    "Testing Model (Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab60068-3311-4ad2-8ed3-f98b00652c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 images along with class labels\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "figure = plt.figure(figsize=(16, 16), facecolor='white')\n",
    "cols, rows = 5, 4\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    if i == 20:\n",
    "        break\n",
    "        \n",
    "    if has_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    input = inputs[0]  # extract a single image from the batch\n",
    "    image = to_pil_image(input.squeeze())  # Convert the tensor back to the PIL image format\n",
    "    image = image.convert('L')   # Convert the image to grayscale mode\n",
    "\n",
    "    # Predict the label\n",
    "    input = convert_to_3channel(input.unsqueeze(0))\n",
    "    output = resnet(input)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    figure.add_subplot(rows, cols, i + 1)\n",
    "    plt.title(\"True:\" + labels_map[labels[0].item()] + \"\\nPredicted:\" + labels_map[predicted[0].item()])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2a544-875f-4379-a1ee-208c6a3d898c",
   "metadata": {},
   "source": [
    "Checkpoint Model Comparison Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc51f3f-3659-40b0-8d7d-b028ae8d88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "scores = {}\n",
    "for model_filename in os.listdir(checkpoint_dir):\n",
    "    match = re.search(\"epoch_(\\d+)\\.pth\", model_filename)\n",
    "    if match:\n",
    "        epoch = int(match.group(1))\n",
    "        model_path = os.path.join(checkpoint_dir, model_filename)\n",
    "        # Load the saved model\n",
    "        resnet = models.resnet18(weights=None)\n",
    "        resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "        resnet.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        # If using GPU, move the model to GPU\n",
    "        if has_cuda:\n",
    "            resnet = resnet.cuda()\n",
    "            \n",
    "        # Set the model to evaluation mode\n",
    "        resnet.eval()\n",
    "\n",
    "        # Evaluation loop\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                if has_cuda:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = convert_to_3channel(inputs)\n",
    "                outputs = resnet(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print the accuracy on the test data\n",
    "        # print(\"Model:\", model_filename)\n",
    "        # print(\"Accuracy on test data: %d %%\" % (100 * correct / total))\n",
    "        scores[epoch] = (100 * correct / total)\n",
    "        \n",
    "# Process the dictionary into two lists\n",
    "epoch_numbers = []\n",
    "accuracies = []\n",
    "for epoch_number, accuracy in sorted(scores.items()):\n",
    "    epoch_numbers.append(epoch_number)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot the accuracy scores of all models\n",
    "plt.plot(epoch_numbers, accuracies)\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492327f6-38e4-49a6-8411-bcaea500e071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
